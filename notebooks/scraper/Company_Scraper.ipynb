{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsyNmdDnifAA"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtOdAVKq2f74",
        "outputId": "63b7fcb3-2864-47d5-e46c-0a119b0926db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Home - NuraLogix\n",
            "Home\n",
            "Technology\n",
            "Security\n",
            "Research\n",
            "Products\n",
            "DeepAffex™\n",
            "Anura™\n",
            "DFX Live™\n",
            "News\n",
            "Company\n",
            "Anura Web\n",
            "Get in Touch\n",
            "Select Page\n",
            "Take a selfie, know your healthie!™\n",
            "Anura’s intended use is to increase people’s awareness of their general wellness.\n",
            "Try Anura™ Lite:\n",
            " For Investigational Use Only. Anura™ is not a substitute for the clinical judgment of a health care professional. Anura™ is intended to improve your awareness of general wellness. Anura™ does not diagnose, treat, mitigate or prevent any disease, symptom, disorder or abnormal physical state. Consult with a health care professional or emergency services if you believe you may have a medical issue.\n",
            "Read our publications:\n",
            "Clinical studies\n",
            "Research\n",
            "Research\n",
            "A Different Kind of AI\n",
            "Using a conventional video camera, our cloud-based Affective AI engine can detect and measure a wide variety of human affects that take the form of physiological and psychological states.\n",
            "SEE HOW\n",
            "Fully Compliant with Global Data Regulations\n",
            "In the News\n",
            "We’re Making Headlines\n",
            "SEE ALL\n",
            "GET IN TOUCH\n",
            "Quick Links\n",
            "Technology\n",
            "Research\n",
            "DeepAffex™\n",
            "Anura™\n",
            "DFX Live™\n",
            "News\n",
            "Company\n",
            "Privacy Policy\n",
            "Terms of Use\n",
            "Cookie Policy\n",
            "Security & Compliance\n",
            "Corporate Headquarters\n",
            "NuraLogix Corporation\n",
            "Toronto, Canada250 University Avenue, Suite 209\n",
            "Toronto, ON, Canada M5H 3E5\n",
            "Email: info@nuralogix.ai\n",
            "Phone: +1 (416) 368-6000\n",
            "In the United States, this product is for Investigational Use Only. The performance characteristics of this product have not been established.\n",
            "© Copyright NuraLogix Corporation\n",
            "Regional Offices\n",
            "North Americasales-na@nuralogix.ai\n",
            "Latin America and EMEA (Europe, Middle East and Africa)sales-emea@nuralogix.ai\n",
            "APAC (Asia-Pacific)sales-apac@nuralogix.ai\n",
            "China (Mainland)sales-china@nuralogix.ai\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from urllib.request import Request, urlopen\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import re\n",
        "\n",
        "url = 'https://www.nuralogix.ai/'\n",
        "\n",
        "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "\n",
        "webpage = urlopen(req).read()\n",
        "page_soup = soup(webpage, \"html.parser\")\n",
        "#print(page_soup.text, type(page_soup.text))\n",
        "print(re.sub(r'\\n\\s*\\n', '\\n', page_soup.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yzpuRg623kX",
        "outputId": "980d8603-88ab-49bb-da88-4cd45a21d033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izCq5k6XEhO0"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s2t73lHEiB1"
      },
      "outputs": [],
      "source": [
        "openai.api_key = '[redacted]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B31UwUV5LkKz",
        "outputId": "e0d54e06-fb48-4bf8-e2e1-64585c0a3f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You asked if Google is a good company.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "res = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a business assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Is Google a good company?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Add the result here as an f string\"},\n",
        "        {\"role\": \"user\", \"content\": \"What did I ask last time?\"}\n",
        "    ]\n",
        ")\n",
        "print(res['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_tqgS_RREt7",
        "outputId": "3afff97a-85ad-4c40-c8b8-65344e1be07b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultimate-sitemap-parser\n",
            "  Downloading ultimate_sitemap_parser-0.5-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from ultimate-sitemap-parser) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from ultimate-sitemap-parser) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->ultimate-sitemap-parser) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->ultimate-sitemap-parser) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->ultimate-sitemap-parser) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->ultimate-sitemap-parser) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->ultimate-sitemap-parser) (2023.7.22)\n",
            "Installing collected packages: ultimate-sitemap-parser\n",
            "Successfully installed ultimate-sitemap-parser-0.5\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Collecting linkedin-scraper\n",
            "  Downloading linkedin_scraper-2.11.2-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from linkedin-scraper) (4.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from linkedin-scraper) (2.31.0)\n",
            "Collecting selenium (from linkedin-scraper)\n",
            "  Downloading selenium-4.11.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->linkedin-scraper) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->linkedin-scraper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->linkedin-scraper) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->linkedin-scraper) (2023.7.22)\n",
            "Collecting trio~=0.17 (from selenium->linkedin-scraper)\n",
            "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium->linkedin-scraper)\n",
            "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->linkedin-scraper) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->linkedin-scraper) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium->linkedin-scraper)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->linkedin-scraper) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->linkedin-scraper) (1.1.3)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->linkedin-scraper)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3<3,>=1.21.1->requests->linkedin-scraper) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium->linkedin-scraper)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium, linkedin-scraper\n",
            "Successfully installed h11-0.14.0 linkedin-scraper-2.11.2 outcome-1.2.0 selenium-4.11.2 trio-0.22.2 trio-websocket-0.10.3 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ultimate-sitemap-parser\n",
        "!pip install requests\n",
        "!pip install linkedin-scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qIuFbK3RGHA"
      },
      "outputs": [],
      "source": [
        "from usp.tree import sitemap_tree_for_homepage\n",
        "# https://understandingdata.com/posts/how-to-easily-find-all-of-the-sitemap-xml-files-in-python/\n",
        "import warnings\n",
        "import sys\n",
        "import os\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "from contextlib import redirect_stdout\n",
        "from linkedin_scraper import Company"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPrM5jqGGqOn"
      },
      "outputs": [],
      "source": [
        "# company_name and company_url are retrieved from the PeopleDataLab\n",
        "# company dataset.\n",
        "# NOTE: company_url must be the complete https link\n",
        "def generate_company_desc(company_name, company_url, num_pages_2_scrape=1, num_pages_2_search=30):\n",
        "  try:\n",
        "    # Use sitemap to get extra data for gpt\n",
        "    tree = sitemap_tree_for_homepage(company_url)\n",
        "\n",
        "    # for page in tree.all_pages():\n",
        "    #   print(page)\n",
        "\n",
        "    # Get urls\n",
        "    urls = [page.url for page in tree.all_pages()]\n",
        "\n",
        "    if len(urls) == 0: raise Exception('Problem with finding sitemap.')\n",
        "\n",
        "    # Extract page document names from urls\n",
        "    pages = [urlparse(url).path.strip('/') for url in urls]\n",
        "    print(pages)\n",
        "    pages = pages[:num_pages_2_search]\n",
        "    # pages_str = ','.join([str(elem) for i, elem in enumerate(pages)])\n",
        "\n",
        "    # Determine best link(s) to get company description\n",
        "    get_link_prompt = f\"\"\"\n",
        "    Given the list of pages sent in the next message in {company_name}'s website,\n",
        "    what are the top {num_pages_2_scrape} page(s) that would contain the most useful data\n",
        "    to generate a general and useful description of the company - closest to the about us page?\n",
        "    \"\"\".replace('\\n', '').replace('    ', '')\n",
        "    # get_link_prompt = f\"\"\"\n",
        "    # Given the list below of pages in {company_name}'s website,\n",
        "    # what are the top {num_pages_2_scrape} page(s) that will contain 'about us', 'products', 'industries' or 'technology' information?\n",
        "    # \"\"\".replace('\\n', '').replace('    ', '')\n",
        "\n",
        "    message = [{\"role\": \"user\", \"content\": page} for page in pages]\n",
        "    message.insert(0, {\"role\": \"assistant\", \"content\": \"Sure!\"})\n",
        "    message.insert(0, {\"role\": \"user\", \"content\": \"I'm gonna send the pages one by one. Okay?\"})\n",
        "    message.insert(0, {\"role\": \"assistant\", \"content\": \"Sure, what are the pages?\"})\n",
        "    message.insert(0, {\"role\": \"user\", \"content\": get_link_prompt})\n",
        "    message.insert(0, {\"role\": \"system\", \"content\": \"You are a research analyst for an investment company.\"})\n",
        "    message.append({\"role\": \"user\", \"content\": \"Now give me the best page.\"})\n",
        "    message.append({\"role\": \"assistant\", \"content\": \"Answer with 'NAME_OF_PAGE'\"})\n",
        "\n",
        "\n",
        "    res = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=message\n",
        "    )\n",
        "\n",
        "    reply = res['choices'][0]['message']['content']\n",
        "\n",
        "    print(reply)\n",
        "\n",
        "  except Exception as e:\n",
        "    # If that doesnt work, use company home page + linkedin description\n",
        "    # NOTE: use linkedin description only if we are using the sample subset of companies\n",
        "\n",
        "    print(e)\n",
        "    # company = Company('https://www.linkedin.com/company/nuralogix-corporation/')\n",
        "\n",
        "    api_key = 'YOUR_API_KEY'\n",
        "    headers = {'Authorization': 'Bearer ' + api_key}\n",
        "    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
        "    linkedin_profile_url = 'https://www.linkedin.com/in/williamhgates'\n",
        "\n",
        "    response = requests.get(api_endpoint,\n",
        "                            params={'url': linkedin_profile_url},\n",
        "                            headers=headers)\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  finally:\n",
        "    pass\n",
        "    # prompt gpt for the description\n",
        "\n",
        "  # for page in tree.all_pages():\n",
        "  #   print(page)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # req = Request(company_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "\n",
        "  # webpage = urlopen(req).read()\n",
        "  # page_soup = soup(webpage, \"html.parser\")\n",
        "  # # data = re.sub(r'\\n\\s*\\n', '\\n', page_soup.text)\n",
        "\n",
        "  # # print(data)\n",
        "\n",
        "  # target_url = None\n",
        "  # # Find the header element(s) using appropriate tags, classes, or IDs\n",
        "  # header_elements = page_soup.find_all('header')\n",
        "\n",
        "  # header = None\n",
        "\n",
        "  # # Iterate through the header elements and extract information\n",
        "  # for header_element in header_elements:\n",
        "  #     # Extract the text content or attributes as needed\n",
        "  #     header_text = header_element.get_text()  # Change to appropriate extraction method\n",
        "  #     header = re.sub(r'\\n\\s*\\n', '\\n', header_text)\n",
        "\n",
        "\n",
        "\n",
        "  # print(res)\n",
        "\n",
        "\n",
        "  # prompt = \"\"\"Provide an objective and concise (at most 4 sentences) description\n",
        "  # of the company {} based on both what you know about the company and the data\n",
        "  # given below from the {} company website.\n",
        "\n",
        "  # {}\n",
        "  # \"\"\".format(company_name, company_name, data)\n",
        "\n",
        "  # print(prompt)\n",
        "\n",
        "  # res = openai.ChatCompletion.create(\n",
        "  # model=\"gpt-3.5-turbo\",\n",
        "  # messages=[\n",
        "  #       {\"role\": \"system\", \"content\": \"You are a research analyst for an investment firm.\"},\n",
        "  #       {\"role\": \"user\", \"content\": prompt},\n",
        "  #       {\"role\": \"assistant\", \"content\": \"Add the result here as an f string\"},\n",
        "  #   ]\n",
        "  # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHVJHQsbFRlN",
        "outputId": "99931d50-bcdb-442d-d5ca-514341cf291d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-20 03:02:36,871 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/robots.txt...\n",
            "2023-08-20 03:02:36,873 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/robots.txt...\n",
            "2023-08-20 03:02:37,206 INFO usp.fetch_parse [772/MainThread]: Parsing sitemap from URL https://www.nuralogix.ai/robots.txt...\n",
            "2023-08-20 03:02:37,208 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/sitemap_index.xml...\n",
            "2023-08-20 03:02:37,213 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/sitemap_index.xml...\n",
            "2023-08-20 03:02:37,594 INFO usp.fetch_parse [772/MainThread]: Parsing sitemap from URL https://www.nuralogix.ai/sitemap_index.xml...\n",
            "2023-08-20 03:02:37,596 INFO usp.fetch_parse [772/MainThread]: Fetching level 1 sitemap from https://xenodochial-johnson9411.on.getshifter.io/post-sitemap.xml...\n",
            "2023-08-20 03:02:37,600 INFO usp.helpers [772/MainThread]: Fetching URL https://xenodochial-johnson9411.on.getshifter.io/post-sitemap.xml...\n",
            "2023-08-20 03:02:38,002 INFO usp.fetch_parse [772/MainThread]: Parsing sitemap from URL https://xenodochial-johnson9411.on.getshifter.io/post-sitemap.xml...\n",
            "2023-08-20 03:02:38,015 INFO usp.fetch_parse [772/MainThread]: Fetching level 1 sitemap from https://xenodochial-johnson9411.on.getshifter.io/page-sitemap.xml...\n",
            "2023-08-20 03:02:38,016 INFO usp.helpers [772/MainThread]: Fetching URL https://xenodochial-johnson9411.on.getshifter.io/page-sitemap.xml...\n",
            "2023-08-20 03:02:38,291 INFO usp.fetch_parse [772/MainThread]: Parsing sitemap from URL https://xenodochial-johnson9411.on.getshifter.io/page-sitemap.xml...\n",
            "2023-08-20 03:02:38,305 INFO usp.fetch_parse [772/MainThread]: Fetching level 1 sitemap from https://xenodochial-johnson9411.on.getshifter.io/project-sitemap.xml...\n",
            "2023-08-20 03:02:38,306 INFO usp.helpers [772/MainThread]: Fetching URL https://xenodochial-johnson9411.on.getshifter.io/project-sitemap.xml...\n",
            "2023-08-20 03:02:38,475 INFO usp.fetch_parse [772/MainThread]: Parsing sitemap from URL https://xenodochial-johnson9411.on.getshifter.io/project-sitemap.xml...\n",
            "2023-08-20 03:02:38,480 INFO usp.fetch_parse [772/MainThread]: Fetching level 1 sitemap from https://xenodochial-johnson9411.on.getshifter.io/category-sitemap.xml...\n",
            "2023-08-20 03:02:38,482 INFO usp.helpers [772/MainThread]: Fetching URL https://xenodochial-johnson9411.on.getshifter.io/category-sitemap.xml...\n",
            "2023-08-20 03:02:38,683 INFO usp.fetch_parse [772/MainThread]: Parsing sitemap from URL https://xenodochial-johnson9411.on.getshifter.io/category-sitemap.xml...\n",
            "2023-08-20 03:02:38,687 INFO usp.fetch_parse [772/MainThread]: Fetching level 1 sitemap from https://xenodochial-johnson9411.on.getshifter.io/author-sitemap.xml...\n",
            "2023-08-20 03:02:38,690 INFO usp.helpers [772/MainThread]: Fetching URL https://xenodochial-johnson9411.on.getshifter.io/author-sitemap.xml...\n",
            "2023-08-20 03:02:38,840 INFO usp.fetch_parse [772/MainThread]: Parsing sitemap from URL https://xenodochial-johnson9411.on.getshifter.io/author-sitemap.xml...\n",
            "2023-08-20 03:02:38,849 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/sitemap-index.xml...\n",
            "2023-08-20 03:02:38,850 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/sitemap-index.xml...\n",
            "2023-08-20 03:02:39,173 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/sitemap-index.xml failed: 404 Not Found\n",
            "2023-08-20 03:02:39,179 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/sitemap-index.xml\n",
            "2023-08-20 03:02:39,184 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/sitemap.xml...\n",
            "2023-08-20 03:02:39,188 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/sitemap.xml...\n",
            "2023-08-20 03:02:39,580 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/sitemap.xml failed: 404 Not Found\n",
            "2023-08-20 03:02:39,582 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/sitemap.xml\n",
            "2023-08-20 03:02:39,583 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/sitemap/sitemap-index.xml...\n",
            "2023-08-20 03:02:39,590 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/sitemap/sitemap-index.xml...\n",
            "2023-08-20 03:02:40,001 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/sitemap/sitemap-index.xml failed: 404 Not Found\n",
            "2023-08-20 03:02:40,003 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/sitemap/sitemap-index.xml\n",
            "2023-08-20 03:02:40,017 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/admin/config/search/xmlsitemap...\n",
            "2023-08-20 03:02:40,022 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/admin/config/search/xmlsitemap...\n",
            "2023-08-20 03:02:40,421 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/admin/config/search/xmlsitemap failed: 404 Not Found\n",
            "2023-08-20 03:02:40,423 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/admin/config/search/xmlsitemap\n",
            "2023-08-20 03:02:40,424 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/sitemap.xml.gz...\n",
            "2023-08-20 03:02:40,432 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/sitemap.xml.gz...\n",
            "2023-08-20 03:02:40,885 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/sitemap.xml.gz failed: 404 Not Found\n",
            "2023-08-20 03:02:40,888 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/sitemap.xml.gz\n",
            "2023-08-20 03:02:40,889 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/sitemap_index.xml.gz...\n",
            "2023-08-20 03:02:40,891 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/sitemap_index.xml.gz...\n",
            "2023-08-20 03:02:41,309 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/sitemap_index.xml.gz failed: 404 Not Found\n",
            "2023-08-20 03:02:41,312 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/sitemap_index.xml.gz\n",
            "2023-08-20 03:02:41,318 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/sitemap...\n",
            "2023-08-20 03:02:41,320 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/sitemap...\n",
            "2023-08-20 03:02:41,744 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/sitemap failed: 404 Not Found\n",
            "2023-08-20 03:02:41,747 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/sitemap\n",
            "2023-08-20 03:02:41,749 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/.sitemap.xml...\n",
            "2023-08-20 03:02:41,752 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/.sitemap.xml...\n",
            "2023-08-20 03:02:42,131 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/.sitemap.xml failed: 404 Not Found\n",
            "2023-08-20 03:02:42,134 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/.sitemap.xml\n",
            "2023-08-20 03:02:42,135 INFO usp.fetch_parse [772/MainThread]: Fetching level 0 sitemap from https://www.nuralogix.ai/sitemap-index.xml.gz...\n",
            "2023-08-20 03:02:42,138 INFO usp.helpers [772/MainThread]: Fetching URL https://www.nuralogix.ai/sitemap-index.xml.gz...\n",
            "2023-08-20 03:02:42,567 WARNING usp.helpers [772/MainThread]: Request for URL https://www.nuralogix.ai/sitemap-index.xml.gz failed: 404 Not Found\n",
            "2023-08-20 03:02:42,583 INFO usp.helpers [772/MainThread]: Not retrying for URL https://www.nuralogix.ai/sitemap-index.xml.gz\n",
            "['anura-international-debut-at-ces-asia-2019', 'nuralogix-returns-alibabas-2050', 'nuralogixs-patent-for-contactless-blood-pressure-determination-is-approved', 'anura-is-now-available-on-ios-and-android', 'smartphone-to-measure-blood-pressure-research-paper-took-over-international-news', 'anura-your-personal-wellness-ai-now-available-on-the-app-store', 'nuralogix-myfiziq-partner-up-to-develop-completescan', 'sanitas-launches-bluau-app-with-anura-technology-a-major-step-forward-in-digital-health', 'nuralogix-anura-contactless-health-vitals-measurement-solution-wins-2021-medtech-breakthrough-award', '', 'technology', 'covid-19-self-assessment', 'blog', 'download-apk', 'ces-2022', 'deep-affex', 'research', 'anuraweb', 'terms-and-conditions', 'privacy-policy-controller', 'privacy-policy-processor', 'health-in-transportation-partnership', 'get-in-touch-privacy', 'get-in-touch', 'insurance-providers-can-assess-customer-health-risks-in-real-time-using-contactless-vital-sign-monitoring-solution-from-nuralogix', 'security-compliance', 'nuralogix-researchers-announce-ai-models-that-can-predict-metabolic-risks', 'nuralogix-researchers-announce-ai-models-that-can-predict-blood-biomarker-risk', 'anura-3', 'dfx-live', 'anura', 'ces', 'nuralogix-announces-holy-grail-of-blood-pressure-measurement', 'ntt-selects-nuralogix', 'agreement-between-telepass-and-neocogita-for-road-safety', 'nuralogix-researchers-announce-the-capability-to-assess-type2-diabetes-and-blood-biomarker-health-issues-using-any-video-enabled-device', 'anura-for-telemedicine', 'anura-for-insurance', 'anura-for-employee-wellness', 'anura-for-business', 'news', 'updates', 'nuralogix-researchers-announce-the-capability-to-predict-risk-for-fatty-liver-disease-and-blood-biomarkers-health-issues-using-any-video-enabled-device', 'cookie-policy', 'privacy-policy', 'terms-of-use', 'company', 'project', 'category/blog', 'author/charlesm', 'author/winshiwong']\n",
            "Based on the list of pages provided, the page that would contain the most useful data to generate a general and useful description of Nuralogix Corporation is the 'anura-international-debut-at-ces-asia-2019' page.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# generate_company_desc('RBC bank', 'https://www.rbcroyalbank.com/personal.html')\n",
        "with redirect_stdout(sys.stderr):\n",
        "  # generate_company_desc('US Army', 'https://www.goarmy.com/')\n",
        "  # generate_company_desc('InfoSys', 'https://infosys.com')\n",
        "  generate_company_desc('Nuralogix Corporation', 'https://www.nuralogix.ai/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yncLgtbFL1KK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
